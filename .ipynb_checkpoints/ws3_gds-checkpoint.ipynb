{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/eliauf23/gateway-data-science/blob/main/ws3_gds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n7-PnXWKY5Db"
   },
   "source": [
    "## Worksheet Week 3\n",
    "### Classification I\n",
    "\n",
    "In this worksheet you will work on a simple binary classification problem based on k-nearest neighbor classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ymgT6eEY5Dp"
   },
   "source": [
    "### PART I: Understanding the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyufXhoHY5Dq"
   },
   "source": [
    "We will work with the Breast Cancer Wisconsin dataset. Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image. \n",
    "\n",
    "For more information, see https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "\n",
    "**0. Up and running**\n",
    "\n",
    "a) Import `numpy`, `matplotlib.pyplot` and `pandas`\n",
    "\n",
    "**1. Load the dataset.**\n",
    "\n",
    "a) You can use sklearn's datasets package, and the function `load_breast_cancer()`\n",
    "\n",
    "b) Turn this data into a DataFrame, and have a look at it (using the functions ``head()`` and ``describe()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F_eFdX6-Y5Dt",
    "outputId": "0bd1fd44-0a8d-49a7-d5d2-e3114175af48"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "data = datasets.load_breast_cancer()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[\"label\"] = data.target\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "id": "fxGwHITIY5D0",
    "outputId": "8efd115d-4353-437a-a4f4-0276cf4809c0"
   },
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "id": "xMEXPJ_yY5D1",
    "outputId": "b57fd169-8208-4931-e10c-975948629ce2"
   },
   "outputs": [],
   "source": [
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNlExk_7Y5D2"
   },
   "source": [
    "**2. Exploring the data**\n",
    "\n",
    "a) How many samples are there? how many features, and what kind are they?\n",
    "\n",
    "569 samples, 31 features, all appear to be real, positive numbers represented as float64 datatype.\n",
    "\n",
    "b) What is the distribution of classes in this dataset?\n",
    "\n",
    "2 classes - malignant & benign. 212 samples are malignant, 357 are benign. \n",
    "\n",
    "c) To gain some more insight into these features, have a look at the distribution of the first four features. An appealing way of this is by computing *violin plots*. Try using the function `violinplot` of the `seaborn` package to display the distributions of the first four features *per class*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jh8s9wqzY5D4",
    "outputId": "70619530-2242-48b9-e3c4-d101356c9d0b"
   },
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ILLCPXxgTwn",
    "outputId": "13cc522f-b9bb-44f8-d262-4c0fae3141d3"
   },
   "outputs": [],
   "source": [
    "df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "DDGgPvnaZEf6",
    "outputId": "3a47064a-a9b7-40a0-d8e8-6d2b183afc0c"
   },
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplot(1, 4, figsize=(15,4))\n",
    "\n",
    "sns.violinplot(data=df, x=\"label\", y=\"mean radius\")\n",
    "sns.violinplot(data=df, x=\"label\", y=\"mean texture\")\n",
    "sns.violinplot( x=\"label\", y=\"mean perimeter\", data=df)\n",
    "sns.violinplot( x=\"label\", y=\"mean area\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "MjHFHF1Pam1V",
    "outputId": "af5c8757-2185-4eae-c130-fb0d7acc19f8"
   },
   "outputs": [],
   "source": [
    "sns.violinplot( x=\"label\", y=\"mean texture\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "3Mm97abPan6R",
    "outputId": "0af02505-ee4a-4070-94fd-8514428541fd"
   },
   "outputs": [],
   "source": [
    "sns.violinplot( x=\"label\", y=\"mean perimeter\", data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "_M5nlprgawRv",
    "outputId": "d733ce90-fc82-4975-8e30-4c1446cbf964"
   },
   "outputs": [],
   "source": [
    "sns.violinplot( x=\"label\", y=\"mean area\", data=df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IelTC-wXY5D5"
   },
   "source": [
    "## Part II: Classification\n",
    "\n",
    "**1. Getting started**\n",
    "\n",
    "a) Split the data into a training and validation set, at a ratio of 70/30. You can do this manually, or use the function `train_test_split()` from the package `sklearn.model_selection`.\n",
    "\n",
    "b) Compute the distribution of positive and negative samples in your training and validation splits. Are they the same as the distribution of the original data? Compare with by using the parameter `stratify` within `train_test_split()`.\n",
    "\n",
    "c) Import the function from `KNeighborsClassifier` from the package `sklearn.neighbors`, and create a classifier with the choice of $k=1$ -- one neighbor. Have a look at https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html for details\n",
    "\n",
    "d) Fit the classifier (use the method `fit()` of the instance you created) to your training data, and then compute accuracy (use the method `score()`) on the validation data. What is the accuracy of this classifier?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kr12bACrY5D7",
    "outputId": "50be1a39-e862-4279-9f1c-8c7d31a2a7e4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = df.label\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, stratify=y)\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "model.fit(x_train, y_train)\n",
    "model.score(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApqxqRgcY5D7"
   },
   "source": [
    "**2. Dependence on amounts of data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CKaCgxxhY5D8"
   },
   "source": [
    "We will now explore how this performance depends on the number training data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHfblXnKY5D9"
   },
   "source": [
    "\n",
    "a) As before, construct a training set and validation set.\n",
    "\n",
    "b) Fit a k-NN classifier with increasing amounts of data, from 1 to $60$, then. For each model, record its training accuracy and validation accuracy.\n",
    "\n",
    "c) Plot the obtained training and validation accuracy as a function of the number of training samples.\n",
    "\n",
    "d) Since your results will depend on the specific (random) draw of your data, repeat points *a* and *b* above 20 times, each drawing a different split of training/validation. Finally, plot the *average* of the accuracies (training and validation) for each number of training samples.\n",
    "\n",
    "e) Why is the training accuracy at 1?\n",
    "\n",
    "f) Run again point *d* above, but now using a k-nearest neighbor classifier with $k=5$. What happened to the training accuracy, and why?\n",
    "\n",
    "g) Noting that the validation accuracy is an unbiased estimate of the Risk of the classifier, what can you say about the value of the Training Error? is it unbiased?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a-N9kRHaY5D-",
    "outputId": "5c4eab81-b277-42d4-d436-f042b8c8063b"
   },
   "outputs": [],
   "source": [
    "# index i in array corresponds to number of data points\n",
    "accuracy = [0] * 60\n",
    "model = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "for j in range(20):\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, stratify=y)\n",
    "  for i in range(1, 60):\n",
    "    x_new = x_train[0:i]\n",
    "    y_new = y_train[0:i]\n",
    "    model.fit(x_new, y_new)\n",
    "    score = model.score(x_test[0:i], y_test[0:i])\n",
    "    #print(\"index \" ,i, \": score = \", score, \"\\n\")\n",
    "    accuracy[i] += score\n",
    "\n",
    "num = np.divide(accuracy, 20)\n",
    "print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "ANXPr8wGVVN1",
    "outputId": "10fb62a3-626f-4a8d-f696-715ae20950c7"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(num, columns=[\"accuracy\"])\n",
    "df.plot(y=\"accuracy\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "N_OuErxFVqFY",
    "outputId": "522ef528-0d27-437c-aa0e-7a8b31610647"
   },
   "outputs": [],
   "source": [
    "accuracy = [0] * 60\n",
    "model = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "for j in range(20):\n",
    "  x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30, stratify=None)\n",
    "  for i in range(5, 60):\n",
    "    x_new = x_train[0:i]\n",
    "    y_new = y_train[0:i]\n",
    "    model.fit(x_new, y_new)\n",
    "    accuracy[i] = accuracy[i] +  model.score(x_test[0:i], y_test[0:i])\n",
    "\n",
    "num = np.divide(accuracy, 20)\n",
    "df = pd.DataFrame(num, columns=[\"accuracy\"])\n",
    "df.plot(y=\"accuracy\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ll0qwyGiY5D_"
   },
   "source": [
    "**3. Number of Neighbors**\n",
    "\n",
    "This classifier has 1 (hyper)parameter that needs tuning: the number of neighbors, $k$. While we will cover model selection later in the class, it is useful for you to start thinking about these questions.\n",
    "\n",
    "We will select this parameter as the one that maximizes performance *on a validation set*. \n",
    "\n",
    "a) Similarly as before, partition your data into a training and validation set, now using 75% for the validation set. Train *different* kNN classifiers (for different values of k) on the training set, and evaluate their performance on the validation set, and plot the validation accuracy as a function of k. Adequate values of K to explore might be `K =  [1,2,3,4,5,6,7,8,9,10,15,20,25,30,35]`.\n",
    "\n",
    "b) Also, as before, one single run will have too high variance (because of the limited number of samples). As you did in 2.d, repeat this process 20 times, and report the mean validation accuracy as a function of $k$. Moreover, plot this mean together with its $5^{th}$ and $95^{th}$ percentiles of the accuracy for each $k$ (consider using the function `fill_between` of `matplotlib.pyplot`, as well as `numpy`'s `percentile` function. (use the `alpha` parameter for some extra aesthetics!)\n",
    "\n",
    "c) What can you conclude about the number k? How does it influence the result? What value k would you choose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqhSnlIMY5EA"
   },
   "outputs": [],
   "source": [
    "x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=0.25, stratify=None)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "ws3_gds.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
